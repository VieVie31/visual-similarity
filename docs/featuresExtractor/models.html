<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>positive-similarity.featuresExtractor.models API documentation</title>
<meta name="description" content="This is the file that will contain the models dictionnary that will be used to compute the feature maps â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>positive-similarity.featuresExtractor.models</code></h1>
</header>
<section id="section-intro">
<p>This is the file that will contain the models dictionnary that will be used to compute the feature maps.</p>
<p>The use of lambda expressions is merely a trick to not load all the models at once when executing
the script. Instead, we load just the lambda expressions (FunctionType), and then in the featureExtractor
we'll loop over all these lambdas expressions and we'll load model by model.</p>
<p>Each value of this dictionnary must be a dict itself with 2 or 3 keys:
model: the lambda expression that will be called to load the model.
layers: List[int or str] ( the layers indices or names of which we want to extract
the output of this model )
transform (optional): torchvision.transforms (the transforms that will be applied to the dataset)
if not present, we'll use the default one in transforms.py file.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    This is the file that will contain the models dictionnary that will be used to compute the feature maps.

    The use of lambda expressions is merely a trick to not load all the models at once when executing
    the script. Instead, we load just the lambda expressions (FunctionType), and then in the featureExtractor
    we&#39;ll loop over all these lambdas expressions and we&#39;ll load model by model.

    Each value of this dictionnary must be a dict itself with 2 or 3 keys:
        model: the lambda expression that will be called to load the model.
        layers: List[int or str] ( the layers indices or names of which we want to extract
                                   the output of this model )
        transform (optional): torchvision.transforms (the transforms that will be applied to the dataset)
                              if not present, we&#39;ll use the default one in transforms.py file.
&#34;&#34;&#34;

import torch
import torchvision.models
import timm
from . import utils

from facenet_pytorch import InceptionResnetV1

models_dict = dict()

# CLIP - OPENAI

clip_rn50x4 = lambda *args: utils.load_clip_model(&#34;RN50x4&#34;, *args)
models_dict[&#34;clip_rn50x4&#34;] = {
    &#34;model&#34;: clip_rn50x4,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;, &#34;attnpool&#34;],
    &#34;transform&#34;: utils.get_clip_transforms(&#34;RN50x4&#34;),
}

clip_rn50 = lambda *args: utils.load_clip_model(&#34;RN50&#34;, *args)
models_dict[&#34;clip_rn50&#34;] = {
    &#34;model&#34;: clip_rn50,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;, &#34;attnpool&#34;],
    &#34;transform&#34;: utils.get_clip_transforms(&#34;RN50&#34;),
}

clip_rn101 = lambda *args: utils.load_clip_model(&#34;RN101&#34;, *args)
models_dict[&#34;clip_rn101&#34;] = {
    &#34;model&#34;: clip_rn101,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;, &#34;attnpool&#34;],
    &#34;transform&#34;: utils.get_clip_transforms(&#34;RN101&#34;),
}

clip_vit32_b = lambda *args: utils.load_clip_model(&#34;ViT-B/32&#34;, *args)
models_dict[&#34;clip_vit32_b&#34;] = {
    &#34;model&#34;: clip_vit32_b,
    &#34;layers&#34;: [&#34;ln_post&#34;],
    &#34;transform&#34;: utils.get_clip_transforms(&#34;ViT-B/32&#34;),
}


# SUPERVISED

# RESNETs

resnet18 = lambda *args: torchvision.models.resnet18(pretrained=True, *args).eval()
models_dict[&#34;resnet18&#34;] = {
    &#34;model&#34;: resnet18,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

resnet34 = lambda *args: torchvision.models.resnet34(pretrained=True, *args).eval()
models_dict[&#34;resnet34&#34;] = {
    &#34;model&#34;: resnet34,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

resnet50 = lambda *args: torchvision.models.resnet50(pretrained=True, *args).eval()
models_dict[&#34;resnet50&#34;] = {
    &#34;model&#34;: resnet50,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

resnet101 = lambda *args: torchvision.models.resnet101(pretrained=True, *args).eval()
models_dict[&#34;resnet101&#34;] = {
    &#34;model&#34;: resnet101,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

resnet152 = lambda *args: torchvision.models.resnet152(pretrained=True, *args).eval()
models_dict[&#34;resnet152&#34;] = {
    &#34;model&#34;: resnet152,
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

# ALEXNET
models_dict[&#34;alexnet&#34;] = {
    &#34;model&#34;: lambda *args: torchvision.models.alexnet(pretrained=True, *args).eval(),
    &#34;layers&#34;: [&#34;features&#34;],
}



# # other efficientnet versions : torch.hub.list(&#39;rwightman/gen-efficientnet-pytorch&#39;)
models_dict[&#34;efficientnet_b0&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;rwightman/gen-efficientnet-pytorch&#34;, &#34;efficientnet_b0&#34;, pretrained=True, *args
    ).eval(),
    &#34;layers&#34;: [-2],
}

models_dict[&#34;efficientnet_b1&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;rwightman/gen-efficientnet-pytorch&#34;, &#34;efficientnet_b1&#34;, pretrained=True, *args
    ).eval(),
    &#34;layers&#34;: [-2],
}

models_dict[&#34;efficientnet_b2&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;rwightman/gen-efficientnet-pytorch&#34;, &#34;efficientnet_b2&#34;, pretrained=True, *args
    ).eval(),
    &#34;layers&#34;: [-2],
}

models_dict[&#34;efficientnet_b3&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;rwightman/gen-efficientnet-pytorch&#34;, &#34;efficientnet_b3&#34;, pretrained=True, *args
    ).eval(), 
    &#34;layers&#34;: [-2],
}

# Big Transfert
models_dict[&#34;BiT-M-R50x1_in21k&#34;] = {
            &#34;model&#34;: lambda *args: timm.create_model(
                    &#34;resnetv2_50x1_bitm_in21k&#34;, pretrained=True, *args
                        ).eval(),
                            &#34;layers&#34;: [6, 44, 93, -6],
                            }
 
 
models_dict[&#34;BiT-M-R50x3_in21k&#34;] = {
            &#34;model&#34;: lambda *args: timm.create_model(
                        &#34;resnetv2_50x3_bitm_in21k&#34;, pretrained=True, *args
                            ).eval(),
                &#34;layers&#34;: [6, 44, 93, -6],
                }
 
models_dict[&#34;BiT-M-R101x1_in21k&#34;] = {
            &#34;model&#34;: lambda *args: timm.create_model(
                        &#34;resnetv2_101x1_bitm_in21k&#34;, pretrained=True, *args
                            ).eval(),
                &#34;layers&#34;: [6, 44, 93, -6],
                }
 
models_dict[&#34;BiT-M-R101x3_in21k&#34;] = {
            &#34;model&#34;: lambda *args: timm.create_model(
                        &#34;resnetv2_101x3_bitm_in21k&#34;, pretrained=True, *args
                            ).eval(),
                &#34;layers&#34;: [6, 44, 93, -6],
                }
 
models_dict[&#34;BiT-M-R152x2_in21k&#34;] = {
            &#34;model&#34;: lambda *args: timm.create_model(
                    &#34;resnetv2_152x2_bitm_in21k&#34;, pretrained=True, *args
                        ).eval(),
                &#34;layers&#34;: [6, 44, 137, -6], # 93+11*4
                }
 
models_dict[&#34;BiT-M-R152x4_in21k&#34;] = {
            &#34;model&#34;: lambda *args: timm.create_model(
                        &#34;resnetv2_152x4_bitm_in21k&#34;, pretrained=True, *args
                            ).eval(),
                &#34;layers&#34;: [6, 44, 137, -6],
                }


# FACENET
models_dict[&#34;facenet&#34;] = {
    &#34;model&#34;: lambda *args: InceptionResnetV1(pretrained=&#34;vggface2&#34;, *args).eval(),
    &#34;layers&#34;: [&#34;block8&#34;],
}



# Vision Transformer
# all versions are available here: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py

models_dict[&#34;ViT&#34;] = {
    &#34;model&#34;: lambda *args: timm.create_model(
        &#34;vit_base_patch16_224&#34;, pretrained=True, *args
    ).eval(),
    &#34;layers&#34;: [-2],
}

# SEMI-SUPERVISED

# facebook semi-supervised models: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models
models_dict[&#34;resnet50_swsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/semi-supervised-ImageNet1K-models&#34;, &#34;resnet50_swsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

models_dict[&#34;resnext101_32x8d_swsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/semi-supervised-ImageNet1K-models&#34;, &#34;resnext101_32x8d_swsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

models_dict[&#34;resnext101_32x16d_swsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/semi-supervised-ImageNet1K-models&#34;, &#34;resnext101_32x16d_swsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
} 





models_dict[&#34;resnet50_ssl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/semi-supervised-ImageNet1K-models&#34;, &#34;resnet50_ssl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

models_dict[&#34;resnext101_32x8d_ssl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/semi-supervised-ImageNet1K-models&#34;, &#34;resnext101_32x8d_ssl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

models_dict[&#34;resnext101_32x8d_ssl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/semi-supervised-ImageNet1K-models&#34;, &#34;resnext101_32x16d_ssl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}



# WEAKLY-SUPERVISED

# https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/
models_dict[&#34;resnext101_32x8d_wsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/WSL-Images&#34;, &#34;resnext101_32x8d_wsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}  

models_dict[&#34;resnext101_32x16d_wsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/WSL-Images&#34;, &#34;resnext101_32x16d_wsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

models_dict[&#34;resnext101_32x32d_wsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/WSL-Images&#34;, &#34;resnext101_32x32d_wsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
} 

models_dict[&#34;resnext101_32x48d_wsl&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/WSL-Images&#34;, &#34;resnext101_32x48d_wsl&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}



# SELF-SUPERVISED

models_dict[&#34;barlow&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/barlowtwins:main&#34;, &#34;resnet50&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

# Dino (SS)
models_dict[&#34;dino_resnet50&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#34;facebookresearch/dino:main&#34;, &#34;dino_resnet50&#34;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;layer1&#34;, &#34;layer2&#34;, &#34;layer3&#34;, &#34;layer4&#34;],
}

models_dict[&#34;deits16&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#39;facebookresearch/dino:main&#39;, &#39;dino_deits16&#39;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;head&#34;],
}

models_dict[&#34;deits8&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#39;facebookresearch/dino:main&#39;, &#39;dino_deits8&#39;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;head&#34;],
}

models_dict[&#34;dino_vitb8&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#39;facebookresearch/dino:main&#39;, &#39;dino_vitb8&#39;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;head&#34;],
}

models_dict[&#34;dino_vitb16&#34;] = {
    &#34;model&#34;: lambda *args: torch.hub.load(
        &#39;facebookresearch/dino:main&#39;, &#39;dino_vitb16&#39;, *args
    ).eval(),
    &#34;layers&#34;: [&#34;head&#34;],
}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="positive-similarity.featuresExtractor.models.clip_rn101"><code class="name flex">
<span>def <span class="ident">clip_rn101</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">clip_rn101 = lambda *args: utils.load_clip_model(&#34;RN101&#34;, *args)</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.clip_rn50"><code class="name flex">
<span>def <span class="ident">clip_rn50</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">clip_rn50 = lambda *args: utils.load_clip_model(&#34;RN50&#34;, *args)</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.clip_rn50x4"><code class="name flex">
<span>def <span class="ident">clip_rn50x4</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">clip_rn50x4 = lambda *args: utils.load_clip_model(&#34;RN50x4&#34;, *args)</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.clip_vit32_b"><code class="name flex">
<span>def <span class="ident">clip_vit32_b</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">clip_vit32_b = lambda *args: utils.load_clip_model(&#34;ViT-B/32&#34;, *args)</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.resnet101"><code class="name flex">
<span>def <span class="ident">resnet101</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">resnet101 = lambda *args: torchvision.models.resnet101(pretrained=True, *args).eval()</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.resnet152"><code class="name flex">
<span>def <span class="ident">resnet152</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">resnet152 = lambda *args: torchvision.models.resnet152(pretrained=True, *args).eval()</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.resnet18"><code class="name flex">
<span>def <span class="ident">resnet18</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">resnet18 = lambda *args: torchvision.models.resnet18(pretrained=True, *args).eval()</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.resnet34"><code class="name flex">
<span>def <span class="ident">resnet34</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">resnet34 = lambda *args: torchvision.models.resnet34(pretrained=True, *args).eval()</code></pre>
</details>
</dd>
<dt id="positive-similarity.featuresExtractor.models.resnet50"><code class="name flex">
<span>def <span class="ident">resnet50</span></span>(<span>*args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">resnet50 = lambda *args: torchvision.models.resnet50(pretrained=True, *args).eval()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="positive-similarity.featuresExtractor" href="index.html">positive-similarity.featuresExtractor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="positive-similarity.featuresExtractor.models.clip_rn101" href="#positive-similarity.featuresExtractor.models.clip_rn101">clip_rn101</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.clip_rn50" href="#positive-similarity.featuresExtractor.models.clip_rn50">clip_rn50</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.clip_rn50x4" href="#positive-similarity.featuresExtractor.models.clip_rn50x4">clip_rn50x4</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.clip_vit32_b" href="#positive-similarity.featuresExtractor.models.clip_vit32_b">clip_vit32_b</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.resnet101" href="#positive-similarity.featuresExtractor.models.resnet101">resnet101</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.resnet152" href="#positive-similarity.featuresExtractor.models.resnet152">resnet152</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.resnet18" href="#positive-similarity.featuresExtractor.models.resnet18">resnet18</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.resnet34" href="#positive-similarity.featuresExtractor.models.resnet34">resnet34</a></code></li>
<li><code><a title="positive-similarity.featuresExtractor.models.resnet50" href="#positive-similarity.featuresExtractor.models.resnet50">resnet50</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>